{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4d44da5-2935-4a93-879b-baa32249c070"
      },
      "source": [
        "Install the Keras Tuner\n",
        "\n",
        "1. **Install Keras Tuner:**\n",
        "    - Use pip to install Keras Tuner\n",
        "2. **Import necessary libraries:**\n",
        "    - Import Keras Tuner, TensorFlow, and Keras modules\n",
        "3. **Load and preprocess the MNIST data set:**\n",
        "    - Load the MNIST data set.\n",
        "    - Normalize the data set by dividing by 255.0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "847a428c-a164-46cb-9d86-7bffa76cce12",
        "outputId": "5c9f17d7-a0df-43d0-b740-4488d5f18700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner==1.4.7\n",
            "  Using cached keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras-tuner==1.4.7) (3.8.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras-tuner==1.4.7) (24.1)\n",
            "Requirement already satisfied: requests in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras-tuner==1.4.7) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras-tuner==1.4.7) (1.0.5)\n",
            "Requirement already satisfied: absl-py in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner==1.4.7) (2.1.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner==1.4.7) (1.26.4)\n",
            "Requirement already satisfied: rich in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner==1.4.7) (13.7.1)\n",
            "Requirement already satisfied: namex in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner==1.4.7) (0.0.7)\n",
            "Requirement already satisfied: h5py in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner==1.4.7) (3.11.0)\n",
            "Requirement already satisfied: optree in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner==1.4.7) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner==1.4.7) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from requests->keras-tuner==1.4.7) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from requests->keras-tuner==1.4.7) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from requests->keras-tuner==1.4.7) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from requests->keras-tuner==1.4.7) (2025.4.26)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from optree->keras->keras-tuner==1.4.7) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from rich->keras->keras-tuner==1.4.7) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from rich->keras->keras-tuner==1.4.7) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner==1.4.7) (0.1.0)\n",
            "Using cached keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "Installing collected packages: keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow==2.16.2\n",
        "!pip install keras-tuner==1.4.7\n",
        "# !pip install numpy<2.0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "60b0d4d6-55ea-42d1-9484-20c278e7ec6f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# Increase recursion limit to prevent potential issues\n",
        "sys.setrecursionlimit(100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2e9ac456-ffc4-42ed-991f-54d67145f36b"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress all Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set TensorFlow log level to suppress warnings and info messages\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all logs, 1 = filter out INFO, 2 = filter out INFO and WARNING, 3 = ERROR only\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd6e5e26-4bb6-4490-993c-2839573f9107",
        "outputId": "1b23f1a7-6c2e-432e-8865-a970917dfe9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_all, y_all), _ = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten and normalize the images\n",
        "x_all = x_all.reshape((x_all.shape[0], -1)).astype(\"float32\") / 255.0\n",
        "\n",
        "# Split into train+val and test (80/20)\n",
        "x_temp, x_test, y_temp, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split train+val into train and validation (75/25 of 80% = 60/20 overall)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8b2d6e0-607d-4654-8fc2-ebaa2e29cfb6",
        "outputId": "40188bd5-8f34-453d-84f7-14bb96ea22f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (60000, 28, 28)\n",
            "Validation data shape: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "print(f'Training data shape: {x_train.shape}')\n",
        "print(f'Validation data shape: {x_val.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb863c30-436f-4f40-a073-34914d09cd00"
      },
      "source": [
        "Defining the model with hyperparameters\n",
        "\n",
        "\n",
        "**Define a model-building function:**\n",
        "- Create a function `build_model` that takes a `HyperParameters` object as input.\n",
        "- Use the `HyperParameters` object to define the number of units in a dense layer and the learning rate for the optimizer.\n",
        "- Compile the model with sparse categorical cross-entropy loss and Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d10483f1-8b0a-4aef-9f73-ef6826a1dc38"
      },
      "outputs": [],
      "source": [
        "# Define a model-building function\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fad9b1-0f80-45b3-88d7-ec6ca7c40496"
      },
      "source": [
        "Configuring the hyperparameter search\n",
        "\n",
        "\n",
        "**Create a RandomSearch Tuner:**\n",
        "- Use the `RandomSearch` class from Keras Tuner.\n",
        "- Specify the model-building function, optimization objective (validation accuracy), number of trials, and directory for storing results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a51ce74a-09be-4efa-b96a-c41f7cc5182e",
        "outputId": "60c3d569-07a0-4803-82c4-4adb14a955fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading Tuner from my_dir\\intro_to_kt\\tuner0.json\n",
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "# Create a RandomSearch Tuner\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "# Display a summary of the search space\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f874963e-4716-415d-8801-9e22e3fb7ccd"
      },
      "source": [
        "Running the hyperparameter search\n",
        "\n",
        "\n",
        "\n",
        "**Run the search:**\n",
        "- Use the `search` method of the tuner.\n",
        "- Pass in the training data, validation data, and the number of epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f66c076-79ba-4b6b-87ab-99b6b258e33e",
        "outputId": "5a56c093-bcf2-426e-83dd-087287ad42a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 02m 53s]\n",
            "val_accuracy: 0.9781999886035919\n",
            "\n",
            "Best val_accuracy So Far: 0.9804500043392181\n",
            "Total elapsed time: 00h 25m 53s\n",
            "Results summary\n",
            "Results in my_dir\\intro_to_kt\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.0009507241678856559\n",
            "Score: 0.9804500043392181\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.0012967304114198843\n",
            "Score: 0.9799000024795532\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.0016465695698696383\n",
            "Score: 0.9781999886035919\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.003216301914364219\n",
            "Score: 0.977400004863739\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.00045130754049079694\n",
            "Score: 0.9766499996185303\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "learning_rate: 0.0008060771879266742\n",
            "Score: 0.9736999869346619\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.005305163878204072\n",
            "Score: 0.9702499806880951\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "learning_rate: 0.00021170184837733148\n",
            "Score: 0.9612999856472015\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.0003683533381590774\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2079, in signbit\n",
            "    tf.constant(0x80000000, dtype=tf.int32),\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OverflowError: Python int too large to convert to C long\n",
            "\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "learning_rate: 0.00020469981930408676\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2079, in signbit\n",
            "    tf.constant(0x80000000, dtype=tf.int32),\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OverflowError: Python int too large to convert to C long\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the hyperparameter search\n",
        "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
        "\n",
        "# Display a summary of the results\n",
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bede143e-8910-4ed9-9c57-f2d0909b4d1a"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "This command runs the hyperparameter search:\n",
        "\n",
        "- **`epochs=5`**: Each trial is trained for 5 epochs.\n",
        "- **`validation_data=(x_val, y_val)`**: The validation data to evaluate the model's performance during the search.\n",
        "\n",
        "After the search is complete, this command displays a summary of the best hyperparameter configurations found during the search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85a5b1c3-2c01-475f-a514-759b77f789f3"
      },
      "source": [
        " Analyzing and using the best hyperparameters\n",
        "\n",
        "\n",
        "\n",
        "**Retrieve the best hyperparameters:**\n",
        "- Use the `get_best_hyperparameters` method to get the best hyperparameters.\n",
        "- Print the optimal values for the hyperparameters.\n",
        "\n",
        "**Build and train the model:**\n",
        "- Build a model using the best hyperparameters.\n",
        "- Train the model on the full training data set and evaluate its performance on the test set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7124821a-7f60-4dca-96f2-754f146d4148",
        "outputId": "bf4169c2-9983-4065-aa81-e3b9f52f8518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The optimal number of units in the first dense layer is 480.\n",
            "\n",
            "The optimal learning rate for the optimizer is 0.0009507241678856559.\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.8837 - loss: 0.3952 - val_accuracy: 0.9642 - val_loss: 0.1191\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9713 - loss: 0.0986 - val_accuracy: 0.9718 - val_loss: 0.0971\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9835 - loss: 0.0558 - val_accuracy: 0.9747 - val_loss: 0.0824\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9881 - loss: 0.0383 - val_accuracy: 0.9757 - val_loss: 0.0845\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9920 - loss: 0.0264 - val_accuracy: 0.9762 - val_loss: 0.0882\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9948 - loss: 0.0182 - val_accuracy: 0.9769 - val_loss: 0.0985\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9952 - loss: 0.0155 - val_accuracy: 0.9746 - val_loss: 0.1011\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.9961 - loss: 0.0132 - val_accuracy: 0.9768 - val_loss: 0.0950\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9965 - loss: 0.0108 - val_accuracy: 0.9741 - val_loss: 0.1068\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.9967 - loss: 0.0101 - val_accuracy: 0.9788 - val_loss: 0.1006\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.1082\n",
            "Test accuracy: 0.9794999957084656\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the best hyperparameters\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"\"\"\n",
        "\n",
        "The optimal number of units in the first dense layer is {best_hps.get('units')}.\n",
        "\n",
        "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Build and Train the Model with Best Hyperparameters\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_val, y_val)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2140dd6-5f9c-4265-a704-cb8a483cf486"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "This code retrieves the best hyperparameters found during the search:\n",
        "\n",
        "- **`get_best_hyperparameters(num_trials=1)`**: Gets the best hyperparameter configuration.\n",
        "- **`print(f\"...\")`**: Prints the best hyperparameters.\n",
        "- **`model.fit(...)`**: Trains the model on the full training data with a validation split of 20%.\n",
        "- **`model.evaluate(...)`**: Evaluates the model on the test (validation) dataset and prints the accuracy, which gives an indication of how well the model generalizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6569e85-7b85-44ae-9772-3f551550cc05"
      },
      "source": [
        " Setting Up Keras Tuner\n",
        "\n",
        "\n",
        "1. Install Keras Tuner.\n",
        "2. Import necessary libraries.\n",
        "3. Load and preprocess the MNIST data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45ffabf1-adbc-4655-b6b4-5d69c1e3b504",
        "outputId": "a8cef716-bd3f-4e2e-b099-c48e3286e619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-tuner in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (1.4.7)\n",
            "Requirement already satisfied: keras in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner) (2.1.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner) (0.0.7)\n",
            "Requirement already satisfied: h5py in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from keras->keras-tuner) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from requests->keras-tuner) (2025.4.26)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from optree->keras->keras-tuner) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from rich->keras->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from rich->keras->keras-tuner) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chris\\anaconda3\\envs\\hulk_1054_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.0)\n",
            "Training data shape: (60000, 28, 28)\n",
            "Validation data shape: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner\n",
        "\n",
        "# Import necessary libraries\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and preprocess the MNIST data set\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "# Print the shapes of the training and validation datasets\n",
        "print(f'Training data shape: {x_train.shape}')\n",
        "print(f'Validation data shape: {x_val.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd7f458c-7c1c-4660-899c-fcfd02ee92fa"
      },
      "source": [
        "Defining the model with hyperparameters\n",
        "\n",
        "1. Define a model-building function that uses the `HyperParameters` object to specify the number of units in a dense layer and the learning rate.\n",
        "2. Compile the model with sparse categorical cross-entropy loss and Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b478a7ce-ff4c-4aaf-a360-0e91cab1e0c1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Define a model-building function\n",
        "def build_model(hp):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6275473d-9012-4d64-a4d7-64a301904516"
      },
      "source": [
        "Configuring the hyperparameter search\n",
        "\n",
        "\n",
        "1. Create a `RandomSearch` tuner using the model-building function.\n",
        "2. Specify the optimization objective, number of trials, and directory for storing results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e381f159-cf43-47a0-bd5e-42cd79232af5",
        "outputId": "673bf5a2-149b-439e-9113-ae5ba1bdcab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading Tuner from my_dir\\intro_to_kt\\tuner0.json\n",
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "# Create a RandomSearch Tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,  # Ensure 'build_model' function is defined from previous code\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "# Display a summary of the search space\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a3e0817-3ab3-4840-b0cd-83ba25170e66"
      },
      "source": [
        "Running the hyperparameter search\n",
        "\n",
        "1. Run the hyperparameter search using the `search` method of the tuner.\n",
        "2. Pass in the training data, validation data, and the number of epochs.\n",
        "3. Display a summary of the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bd2184b-56e0-4f10-8eab-18e0d151f83e",
        "outputId": "33918914-3d10-49fc-877c-c14f687c3532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in my_dir\\intro_to_kt\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.0009507241678856559\n",
            "Score: 0.9804500043392181\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.0012967304114198843\n",
            "Score: 0.9799000024795532\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.0016465695698696383\n",
            "Score: 0.9781999886035919\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.003216301914364219\n",
            "Score: 0.977400004863739\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.00045130754049079694\n",
            "Score: 0.9766499996185303\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "learning_rate: 0.0008060771879266742\n",
            "Score: 0.9736999869346619\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.005305163878204072\n",
            "Score: 0.9702499806880951\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "learning_rate: 0.00021170184837733148\n",
            "Score: 0.9612999856472015\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.0003683533381590774\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2079, in signbit\n",
            "    tf.constant(0x80000000, dtype=tf.int32),\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OverflowError: Python int too large to convert to C long\n",
            "\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "learning_rate: 0.00020469981930408676\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"c:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2079, in signbit\n",
            "    tf.constant(0x80000000, dtype=tf.int32),\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OverflowError: Python int too large to convert to C long\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#  Run the hyperparameter search\n",
        "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
        "\n",
        "# Display a summary of the results\n",
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4d307e7-3765-4920-a1e1-2cf9b1f38965"
      },
      "source": [
        " Analyzing and using the best hyperparameters\n",
        "\n",
        "\n",
        "1. Retrieve the best hyperparameters using the `get_best_hyperparameters` method.\n",
        "2. Build a model using the best hyperparameters.\n",
        "3. Train the model on the full training data set and evaluate its performance on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b2f74ec-6bab-4ece-b88b-185ba46a4e9b",
        "outputId": "76883d99-b2ee-4fa1-ae9b-e702423bb583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The optimal number of units in the first dense layer is 480.\n",
            "The optimal learning rate for the optimizer is 0.0009507241678856559.\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 15ms/step - accuracy: 0.8895 - loss: 0.3808 - val_accuracy: 0.9619 - val_loss: 0.1300\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 19ms/step - accuracy: 0.9712 - loss: 0.0964 - val_accuracy: 0.9747 - val_loss: 0.0848\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9826 - loss: 0.0571 - val_accuracy: 0.9755 - val_loss: 0.0828\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.9892 - loss: 0.0363 - val_accuracy: 0.9773 - val_loss: 0.0757\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - accuracy: 0.9922 - loss: 0.0274 - val_accuracy: 0.9762 - val_loss: 0.0863\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9941 - loss: 0.0190 - val_accuracy: 0.9772 - val_loss: 0.0886\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 16ms/step - accuracy: 0.9946 - loss: 0.0166 - val_accuracy: 0.9774 - val_loss: 0.0901\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.9967 - loss: 0.0111 - val_accuracy: 0.9778 - val_loss: 0.0871\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 18ms/step - accuracy: 0.9961 - loss: 0.0107 - val_accuracy: 0.9761 - val_loss: 0.1100\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - accuracy: 0.9976 - loss: 0.0086 - val_accuracy: 0.9770 - val_loss: 0.1069\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9771 - loss: 0.1043\n",
            "Validation accuracy: 0.9797000288963318\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The optimal number of units in the first dense layer is {best_hps.get('units')}.\n",
        "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        " # Build and train the model with best hyperparameters\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        " # Evaluate the model on the validation set\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val)\n",
        "\n",
        "print(f'Validation accuracy: {val_acc}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hulk_1054_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "prev_pub_hash": "4480f294d0e5af91350ef1c70e7b3bd8f76b50e8822bcb90342d59ff1810e228"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
