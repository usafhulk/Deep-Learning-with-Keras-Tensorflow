{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceaa1ebd-849b-4751-8ca6-34d3dafe5718"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow numpy"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "728f25e0-fb12-49a0-a9cb-88312e48f5bf",
        "outputId": "6007510f-df12-4090-a9fc-82f699429eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import numpy as np\n",
        "\n",
        "# Suppress all Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set TensorFlow log level to suppress warnings and info messages\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Step 1: Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2233472b-ec3c-45ba-a190-8086c8f61a6a"
      },
      "source": [
        "\n",
        "\n",
        "Create a simple neural network model with a Flatten layer followed by two Dense layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9ecb75e-7548-464d-bf40-d7bbff1a64e2"
      },
      "outputs": [],
      "source": [
        "# Define the Model\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10)\n",
        "])\n"
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39da66e3-1717-40ed-a8b5-413bb16dad08"
      },
      "source": [
        "Define Loss Function and Optimizer:\n",
        "\n",
        "- Use Sparse Categorical Crossentropy for the loss function.\n",
        "- Use the Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb5839c9-2bc1-41bd-a0cd-8b1fe65f2c7f"
      },
      "outputs": [],
      "source": [
        "# Define Loss Function and Optimizer\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n"
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c75bf9b-6e3e-4246-a718-51bc4122d618"
      },
      "source": [
        "Implement the Custom Training Loop:\n",
        "\n",
        "- Iterate over the dataset for a specified number of epochs.\n",
        "- Compute the loss and apply gradients to update the model's weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80afaf2c-1e40-4146-8672-d1b59b32ed91",
        "outputId": "e313acf4-af08-4ac5-dd7a-8e850a99d1fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Step 0: Loss = 2.342745304107666\n",
            "Epoch 1 Step 200: Loss = 0.4407643973827362\n",
            "Epoch 1 Step 400: Loss = 0.16888663172721863\n",
            "Epoch 1 Step 600: Loss = 0.1717824637889862\n",
            "Epoch 1 Step 800: Loss = 0.1786532998085022\n",
            "Epoch 1 Step 1000: Loss = 0.4041965901851654\n",
            "Epoch 1 Step 1200: Loss = 0.2196640968322754\n",
            "Epoch 1 Step 1400: Loss = 0.25246381759643555\n",
            "Epoch 1 Step 1600: Loss = 0.229383185505867\n",
            "Epoch 1 Step 1800: Loss = 0.18786725401878357\n",
            "Start of epoch 2\n",
            "Epoch 2 Step 0: Loss = 0.07759848237037659\n",
            "Epoch 2 Step 200: Loss = 0.21298855543136597\n",
            "Epoch 2 Step 400: Loss = 0.12948989868164062\n",
            "Epoch 2 Step 600: Loss = 0.037624541670084\n",
            "Epoch 2 Step 800: Loss = 0.11023370921611786\n",
            "Epoch 2 Step 1000: Loss = 0.1761862188577652\n",
            "Epoch 2 Step 1200: Loss = 0.13388654589653015\n",
            "Epoch 2 Step 1400: Loss = 0.16112855076789856\n",
            "Epoch 2 Step 1600: Loss = 0.17073768377304077\n",
            "Epoch 2 Step 1800: Loss = 0.09309744089841843\n"
          ]
        }
      ],
      "source": [
        "# Implement the Custom Training Loop\n",
        "\n",
        "epochs = 2\n",
        "# train_dataset = train_dataset.repeat(epochs)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)  # Forward pass\n",
        "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
        "\n",
        "        # Compute gradients and update weights\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Logging the loss every 200 steps\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "238c0013-3e17-4937-a602-217111db3f22"
      },
      "source": [
        "Adding Accuracy Metric:\n",
        "\n",
        "Enhance the custom training loop by adding an accuracy metric to monitor model performance.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b891a118-9ed8-4806-929c-baf71a646d09"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Step 1: Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Create a batched dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9cfea84-f1df-407d-9f84-0adb91e3bba7"
      },
      "source": [
        "Define the Model:\n",
        "Use the same model as earlier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7263943a-fcaf-4b16-b24c-25f80911fe78"
      },
      "outputs": [],
      "source": [
        "#  Define the Model\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
        "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
        "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
        "])\n"
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bacfd4-5dbb-4f93-a388-bfdd1da07c59"
      },
      "source": [
        "Define the loss function, optimizer, and metric:\n",
        "\n",
        "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer.\n",
        "\n",
        "- Add Sparse Categorical Accuracy as a metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b425d918-3140-4e52-9e61-14e7b192ae7b"
      },
      "outputs": [],
      "source": [
        "#  Define Loss Function, Optimizer, and Metric\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
        "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
        "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bba0bb46-7671-4192-b624-8b40e3bc8a00"
      },
      "source": [
        "Implement the custom training loop with accuracy:\n",
        "\n",
        "Track the accuracy during training and print it at regular intervals.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bb3041b-89b1-41f5-a557-4b9fad10236a",
        "outputId": "f4fe8d52-cd67-405e-f13e-31d890e02929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Step 0: Loss = 2.3675007820129395 Accuracy = 0.0625\n",
            "Epoch 1 Step 200: Loss = 0.40460026264190674 Accuracy = 0.832866907119751\n",
            "Epoch 1 Step 400: Loss = 0.19316300749778748 Accuracy = 0.8666614890098572\n",
            "Epoch 1 Step 600: Loss = 0.16448861360549927 Accuracy = 0.8831114768981934\n",
            "Epoch 1 Step 800: Loss = 0.15014925599098206 Accuracy = 0.8952481150627136\n",
            "Epoch 1 Step 1000: Loss = 0.4227451682090759 Accuracy = 0.9023164510726929\n",
            "Epoch 1 Step 1200: Loss = 0.17295680940151215 Accuracy = 0.9090081453323364\n",
            "Epoch 1 Step 1400: Loss = 0.23636934161186218 Accuracy = 0.9143692255020142\n",
            "Epoch 1 Step 1600: Loss = 0.19532684981822968 Accuracy = 0.9173368215560913\n",
            "Epoch 1 Step 1800: Loss = 0.1667139232158661 Accuracy = 0.9212590456008911\n",
            "Start of epoch 2\n",
            "Epoch 2 Step 0: Loss = 0.1011013612151146 Accuracy = 0.96875\n",
            "Epoch 2 Step 200: Loss = 0.15126866102218628 Accuracy = 0.9619091749191284\n",
            "Epoch 2 Step 400: Loss = 0.09804297983646393 Accuracy = 0.9586970210075378\n",
            "Epoch 2 Step 600: Loss = 0.06193074584007263 Accuracy = 0.9605345129966736\n",
            "Epoch 2 Step 800: Loss = 0.12024105340242386 Accuracy = 0.9612203240394592\n",
            "Epoch 2 Step 1000: Loss = 0.24536782503128052 Accuracy = 0.9609140753746033\n",
            "Epoch 2 Step 1200: Loss = 0.08138908445835114 Accuracy = 0.9619848132133484\n",
            "Epoch 2 Step 1400: Loss = 0.11419805884361267 Accuracy = 0.9630398154258728\n",
            "Epoch 2 Step 1600: Loss = 0.12181826680898666 Accuracy = 0.9629723429679871\n",
            "Epoch 2 Step 1800: Loss = 0.08681150525808334 Accuracy = 0.9635618925094604\n",
            "Start of epoch 3\n",
            "Epoch 3 Step 0: Loss = 0.05233779922127724 Accuracy = 1.0\n",
            "Epoch 3 Step 200: Loss = 0.06483759731054306 Accuracy = 0.9746579527854919\n",
            "Epoch 3 Step 400: Loss = 0.09057999402284622 Accuracy = 0.9724127054214478\n",
            "Epoch 3 Step 600: Loss = 0.032345760613679886 Accuracy = 0.9736896753311157\n",
            "Epoch 3 Step 800: Loss = 0.0808539092540741 Accuracy = 0.9738998413085938\n",
            "Epoch 3 Step 1000: Loss = 0.16829177737236023 Accuracy = 0.9737138152122498\n",
            "Epoch 3 Step 1200: Loss = 0.0807175263762474 Accuracy = 0.9741881489753723\n",
            "Epoch 3 Step 1400: Loss = 0.04940849542617798 Accuracy = 0.9744378924369812\n",
            "Epoch 3 Step 1600: Loss = 0.07251431792974472 Accuracy = 0.9740591645240784\n",
            "Epoch 3 Step 1800: Loss = 0.07085657119750977 Accuracy = 0.9747015833854675\n",
            "Start of epoch 4\n",
            "Epoch 4 Step 0: Loss = 0.03392702713608742 Accuracy = 1.0\n",
            "Epoch 4 Step 200: Loss = 0.040404848754405975 Accuracy = 0.9805659055709839\n",
            "Epoch 4 Step 400: Loss = 0.05981656536459923 Accuracy = 0.9801278114318848\n",
            "Epoch 4 Step 600: Loss = 0.0337170735001564 Accuracy = 0.9806572198867798\n",
            "Epoch 4 Step 800: Loss = 0.07143975049257278 Accuracy = 0.980688214302063\n",
            "Epoch 4 Step 1000: Loss = 0.12534783780574799 Accuracy = 0.9811126589775085\n",
            "Epoch 4 Step 1200: Loss = 0.06285949051380157 Accuracy = 0.9809793829917908\n",
            "Epoch 4 Step 1400: Loss = 0.025243297219276428 Accuracy = 0.981107234954834\n",
            "Epoch 4 Step 1600: Loss = 0.050093624740839005 Accuracy = 0.9810274839401245\n",
            "Epoch 4 Step 1800: Loss = 0.036487288773059845 Accuracy = 0.9815380573272705\n",
            "Start of epoch 5\n",
            "Epoch 5 Step 0: Loss = 0.023676160722970963 Accuracy = 1.0\n",
            "Epoch 5 Step 200: Loss = 0.034203123301267624 Accuracy = 0.9877176880836487\n",
            "Epoch 5 Step 400: Loss = 0.0420747771859169 Accuracy = 0.9869856834411621\n",
            "Epoch 5 Step 600: Loss = 0.03467365726828575 Accuracy = 0.9871568083763123\n",
            "Epoch 5 Step 800: Loss = 0.051296401768922806 Accuracy = 0.9868133664131165\n",
            "Epoch 5 Step 1000: Loss = 0.10615893453359604 Accuracy = 0.9869193434715271\n",
            "Epoch 5 Step 1200: Loss = 0.055869147181510925 Accuracy = 0.9867298007011414\n",
            "Epoch 5 Step 1400: Loss = 0.011995896697044373 Accuracy = 0.9866390228271484\n",
            "Epoch 5 Step 1600: Loss = 0.04857306182384491 Accuracy = 0.9864733219146729\n",
            "Epoch 5 Step 1800: Loss = 0.03102918714284897 Accuracy = 0.9868649244308472\n"
          ]
        }
      ],
      "source": [
        "# Implement the Custom Training Loop with Accuracy\n",
        "\n",
        "epochs = 5  # Number of epochs for training\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass: Compute predictions\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            # Compute loss\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # Compute gradients\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        # Apply gradients to update model weights\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update the accuracy metric\n",
        "        accuracy_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log the loss and accuracy every 200 steps\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
        "\n",
        "    # Reset the metric at the end of each epoch\n",
        "    accuracy_metric.reset_state()\n"
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aad4044-971c-4c8d-bd28-d8fc21cdaba5"
      },
      "source": [
        "Custom Callback for Advanced Logging:\n",
        "\n",
        "Implement a custom callback to log additional metrics and information during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dc7593c-59fe-4d2b-ae24-04277b43ba83"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Create a batched dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e17f468-52d3-4f74-9160-49244766c548"
      },
      "source": [
        "Define the Model:\n",
        "\n",
        "Use the same model as earlier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7322571-c9f4-4ecb-b57a-a2051660692f"
      },
      "outputs": [],
      "source": [
        "#  Define the Model\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Flatten the input to a 1D vector\n",
        "    Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
        "    Dense(10)  # Output layer with 10 neurons for the 10 classes (digits 0-9)\n",
        "])\n"
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f674612-bef3-4945-b39c-c11e7685ac5b"
      },
      "source": [
        "Define Loss Function, Optimizer, and Metric:\n",
        "\n",
        "- Use Sparse Categorical Crossentropy for the loss function and Adam optimizer.\n",
        "\n",
        "- Add Sparse Categorical Accuracy as a metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c07e5957-4fca-45ff-aca2-597a30303f1c"
      },
      "outputs": [],
      "source": [
        "# Define Loss Function, Optimizer, and Metric\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
        "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
        "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e6cf7ff-9484-4bca-a1fe-3dc5641bfdd7"
      },
      "source": [
        "Implement the custom training loop with custom callback:\n",
        "\n",
        "Create a custom callback to log additional metrics at the end of each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab211a00-900c-4d4a-bbec-4b189d4152d1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "# Implement the Custom Callback\n",
        "class CustomCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n"
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96faf68c-e238-4c03-a37c-0a05e90e15c5",
        "outputId": "ffa0f2dc-e94c-4e38-e8ba-1f0d5a7a1d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Step 0: Loss = 2.3673155307769775 Accuracy = 0.09375\n",
            "Epoch 1 Step 200: Loss = 0.34604182839393616 Accuracy = 0.832866907119751\n",
            "Epoch 1 Step 400: Loss = 0.19698874652385712 Accuracy = 0.8679083585739136\n",
            "Epoch 1 Step 600: Loss = 0.16240321099758148 Accuracy = 0.8839954137802124\n",
            "Epoch 1 Step 800: Loss = 0.1640537977218628 Accuracy = 0.8972378373146057\n",
            "Epoch 1 Step 1000: Loss = 0.44047534465789795 Accuracy = 0.9045642018318176\n",
            "Epoch 1 Step 1200: Loss = 0.1672089397907257 Accuracy = 0.9111157655715942\n",
            "Epoch 1 Step 1400: Loss = 0.2854663133621216 Accuracy = 0.915752112865448\n",
            "Epoch 1 Step 1600: Loss = 0.22955010831356049 Accuracy = 0.9190349578857422\n",
            "Epoch 1 Step 1800: Loss = 0.19465573132038116 Accuracy = 0.9230288863182068\n",
            "End of epoch 1, loss: 0.031704697757959366, accuracy: 0.9248666763305664\n",
            "Start of epoch 2\n",
            "Epoch 2 Step 0: Loss = 0.07026565819978714 Accuracy = 1.0\n",
            "Epoch 2 Step 200: Loss = 0.1476096659898758 Accuracy = 0.9625310897827148\n",
            "Epoch 2 Step 400: Loss = 0.13121293485164642 Accuracy = 0.9586970210075378\n",
            "Epoch 2 Step 600: Loss = 0.08360129594802856 Accuracy = 0.9596505761146545\n",
            "Epoch 2 Step 800: Loss = 0.13067300617694855 Accuracy = 0.9611813426017761\n",
            "Epoch 2 Step 1000: Loss = 0.3962016999721527 Accuracy = 0.962037980556488\n",
            "Epoch 2 Step 1200: Loss = 0.090269073843956 Accuracy = 0.962713360786438\n",
            "Epoch 2 Step 1400: Loss = 0.20905917882919312 Accuracy = 0.9635081887245178\n",
            "Epoch 2 Step 1600: Loss = 0.14201010763645172 Accuracy = 0.9636750221252441\n",
            "Epoch 2 Step 1800: Loss = 0.11899106204509735 Accuracy = 0.9643080234527588\n",
            "End of epoch 2, loss: 0.027993664145469666, accuracy: 0.9650166630744934\n"
          ]
        }
      ],
      "source": [
        "# Implement the Custom Training Loop with Custom Callback\n",
        "\n",
        "epochs = 2\n",
        "custom_callback = CustomCallback()  # Initialize the custom callback\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass: Compute predictions\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            # Compute loss\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # Compute gradients\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        # Apply gradients to update model weights\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update the accuracy metric\n",
        "        accuracy_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log the loss and accuracy every 200 steps\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
        "\n",
        "    # Call the custom callback at the end of each epoch\n",
        "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
        "\n",
        "    # Reset the metric at the end of each epoch\n",
        "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ee821e6-3cd8-40f0-ac75-dbfe945d8a61"
      },
      "source": [
        "Implement the custom training loop with custom callback:\n",
        "\n",
        "Create a custom callback to log additional metrics at the end of each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b287da11-9d49-4168-b6f2-0bfd700436d9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
        "\n",
        "# Define hidden layers\n",
        "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
        "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation\n"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc8f1d08-049b-48b9-ab13-6fa1e8dbb277"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "`Dense(64, activation='relu')` creates a dense (fully connected) layer with 64 units and ReLU activation function.\n",
        "\n",
        "Each hidden layer takes the output of the previous layer as its input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b18fcbc-2eaa-4bc2-bcb3-9f7de03b6ad6"
      },
      "source": [
        "Define the output layer\n",
        "\n",
        "define the output layer. Suppose you are working on a binary classification problem, so the output layer will have one unit with a sigmoid activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5086b813-b03b-477f-84b3-ab8bb9710092"
      },
      "outputs": [],
      "source": [
        "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8c8f85-6bcf-467f-a2c3-1b673e7e1c48"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "`Dense(1, activation='sigmoid')` creates a dense layer with 1 unit and a sigmoid activation function, suitable for binary classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ff495f0-981d-47ff-8d39-5472a7e6fc41"
      },
      "source": [
        "Create the Model <br/>\n",
        "\n",
        "create the model by specifying the input and output layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cfab49b-9c28-4bfb-8b63-b7d948d71720"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac3ab09e-86b9-4735-aa26-fe59132510e0"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "`Model(inputs=input_layer, outputs=output_layer)` creates a Keras model that connects the input layer to the output layer through the hidden layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c69cc5f9-f744-4b96-bd64-5dc8e6888d62"
      },
      "source": [
        "Compile the Model\n",
        "\n",
        "Before training the model, it needs to be compiled. specify the loss function, optimizer, and evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56d0280a-cad5-4baa-ac39-09d61500c54d"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a61da84d-5d34-40fa-b58b-c7b5d0f8ffea"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "`optimizer='adam'` specifies the Adam optimizer, a popular choice for training neural networks.\n",
        "\n",
        "`loss='binary_crossentropy'` specifies the loss function for binary classification problems.\n",
        "\n",
        "`metrics=['accuracy']` tells Keras to evaluate the model using accuracy during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4701696-b4fb-4379-8d66-124b94e66def"
      },
      "source": [
        "Train the Model\n",
        "\n",
        "train the model on some training data.  `X_train` is our training input data and `y_train` is the corresponding labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a897aeb1-e49e-4b02-a912-ff9617024faa",
        "outputId": "6b715462-0a73-4a00-8db3-e12e2615baa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5048 - loss: 0.6986\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5139 - loss: 0.6976\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5709 - loss: 0.6882\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5334 - loss: 0.6883\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5530 - loss: 0.6873\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5792 - loss: 0.6833\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5780 - loss: 0.6787\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5575 - loss: 0.6761\n",
            "Epoch 9/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5511 - loss: 0.6818\n",
            "Epoch 10/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6069 - loss: 0.6746\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x797b90878210>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "import numpy as np\n",
        "\n",
        "#  Redefine the Model for 20 features\n",
        "model = Sequential([\n",
        "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
        "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#  Generate Example Data\n",
        "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
        "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
        "\n",
        "#  Train the Model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)"
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0f0abe8-dffd-4a61-b8a3-9cbb7b714b87"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "`X_train` and `y_train` are placeholders for your actual training data.\n",
        "\n",
        "`model.fit` trains the model for a specified number of epochs and batch size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73af7f88-da28-4fc1-9baf-24fe46aa1d74"
      },
      "source": [
        "Evaluate the Model\n",
        "\n",
        "After training, evaluate the model on test data to see how well it performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "714c0880-de6d-46d1-baea-f6c906ce1ecb",
        "outputId": "6e4e0eb4-2ebb-4e3b-afc5-30a245050c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4940 - loss: 0.7034  \n",
            "Test loss: 0.7023367285728455\n",
            "Test accuracy: 0.5099999904632568\n"
          ]
        }
      ],
      "source": [
        "# Example test data (in practice, use real dataset)\n",
        "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
        "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels (0 or 1)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Print test loss and accuracy\n",
        "print(f'Test loss: {loss}')\n",
        "print(f'Test accuracy: {accuracy}')\n"
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b57a228-968f-4d52-8ba5-ff9f58bd3235"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "`model.evaluate` computes the loss and accuracy of the model on test data.\n",
        "\n",
        "`X_test` and `y_test` are placeholders for your actual test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3716157-70c1-4ca7-bf20-3761c355cf75"
      },
      "source": [
        " Basic Custom Training Loop\n",
        "\n",
        "\n",
        "\n",
        "- Set up the environment and load the dataset.\n",
        "\n",
        "- Define the model with a Flatten layer and two Dense layers.\n",
        "\n",
        "- Define the loss function and optimizer.\n",
        "\n",
        "- Implement a custom training loop to iterate over the dataset, compute the loss, and update the model's weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "\n",
        "# Define the model with a Flatten layer and two Dense layers\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10)\n",
        "])\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Implement a custom training loop to iterate over the dataset, compute the loss, and update the model's weights\n",
        "epochs = 5\n",
        "# train_dataset = train_dataset.repeat(epochs)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True) # forward pass\n",
        "            loss_value = loss_fn(y_batch_train, logits) # compute loss\n",
        "\n",
        "        # compute gradients and update weights\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Logging the loss every 100 stepss\n",
        "        if step % 100 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iexyXlnMCO7",
        "outputId": "d1e6cb4e-5d63-485f-cfce-aa96e5895c2c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Step 0: Loss = 2.3399946689605713\n",
            "Epoch 1 Step 100: Loss = 0.581192135810852\n",
            "Epoch 1 Step 200: Loss = 0.35511648654937744\n",
            "Epoch 1 Step 300: Loss = 0.25517189502716064\n",
            "Epoch 1 Step 400: Loss = 0.17661848664283752\n",
            "Epoch 1 Step 500: Loss = 0.3493261933326721\n",
            "Epoch 1 Step 600: Loss = 0.16815727949142456\n",
            "Epoch 1 Step 700: Loss = 0.1142866462469101\n",
            "Epoch 1 Step 800: Loss = 0.15714596211910248\n",
            "Epoch 1 Step 900: Loss = 0.10838490724563599\n",
            "Epoch 1 Step 1000: Loss = 0.4504319429397583\n",
            "Epoch 1 Step 1100: Loss = 0.16053643822669983\n",
            "Epoch 1 Step 1200: Loss = 0.16539366543293\n",
            "Epoch 1 Step 1300: Loss = 0.17619968950748444\n",
            "Epoch 1 Step 1400: Loss = 0.2547246515750885\n",
            "Epoch 1 Step 1500: Loss = 0.1651730239391327\n",
            "Epoch 1 Step 1600: Loss = 0.21836332976818085\n",
            "Epoch 1 Step 1700: Loss = 0.08938762545585632\n",
            "Epoch 1 Step 1800: Loss = 0.16124871373176575\n",
            "Start of epoch 2\n",
            "Epoch 2 Step 0: Loss = 0.06793273985385895\n",
            "Epoch 2 Step 100: Loss = 0.21987612545490265\n",
            "Epoch 2 Step 200: Loss = 0.13122038543224335\n",
            "Epoch 2 Step 300: Loss = 0.05858200043439865\n",
            "Epoch 2 Step 400: Loss = 0.1300128549337387\n",
            "Epoch 2 Step 500: Loss = 0.16055037081241608\n",
            "Epoch 2 Step 600: Loss = 0.052467770874500275\n",
            "Epoch 2 Step 700: Loss = 0.028664490208029747\n",
            "Epoch 2 Step 800: Loss = 0.0820312425494194\n",
            "Epoch 2 Step 900: Loss = 0.028760304674506187\n",
            "Epoch 2 Step 1000: Loss = 0.2421765774488449\n",
            "Epoch 2 Step 1100: Loss = 0.13737837970256805\n",
            "Epoch 2 Step 1200: Loss = 0.11161361634731293\n",
            "Epoch 2 Step 1300: Loss = 0.07790840417146683\n",
            "Epoch 2 Step 1400: Loss = 0.14024518430233002\n",
            "Epoch 2 Step 1500: Loss = 0.1273403912782669\n",
            "Epoch 2 Step 1600: Loss = 0.1631053239107132\n",
            "Epoch 2 Step 1700: Loss = 0.026208218187093735\n",
            "Epoch 2 Step 1800: Loss = 0.0935797467827797\n",
            "Start of epoch 3\n",
            "Epoch 3 Step 0: Loss = 0.04305877536535263\n",
            "Epoch 3 Step 100: Loss = 0.09476775676012039\n",
            "Epoch 3 Step 200: Loss = 0.08785471320152283\n",
            "Epoch 3 Step 300: Loss = 0.030133403837680817\n",
            "Epoch 3 Step 400: Loss = 0.09089122712612152\n",
            "Epoch 3 Step 500: Loss = 0.10109854489564896\n",
            "Epoch 3 Step 600: Loss = 0.04085221514105797\n",
            "Epoch 3 Step 700: Loss = 0.012098168022930622\n",
            "Epoch 3 Step 800: Loss = 0.05658780038356781\n",
            "Epoch 3 Step 900: Loss = 0.011363227851688862\n",
            "Epoch 3 Step 1000: Loss = 0.10732626914978027\n",
            "Epoch 3 Step 1100: Loss = 0.08524372428655624\n",
            "Epoch 3 Step 1200: Loss = 0.07086078077554703\n",
            "Epoch 3 Step 1300: Loss = 0.05290888249874115\n",
            "Epoch 3 Step 1400: Loss = 0.08729947358369827\n",
            "Epoch 3 Step 1500: Loss = 0.0873916745185852\n",
            "Epoch 3 Step 1600: Loss = 0.08652318269014359\n",
            "Epoch 3 Step 1700: Loss = 0.01781332865357399\n",
            "Epoch 3 Step 1800: Loss = 0.08212528377771378\n",
            "Start of epoch 4\n",
            "Epoch 4 Step 0: Loss = 0.02741093374788761\n",
            "Epoch 4 Step 100: Loss = 0.0449104905128479\n",
            "Epoch 4 Step 200: Loss = 0.04873964563012123\n",
            "Epoch 4 Step 300: Loss = 0.014475422911345959\n",
            "Epoch 4 Step 400: Loss = 0.08080615103244781\n",
            "Epoch 4 Step 500: Loss = 0.06690914928913116\n",
            "Epoch 4 Step 600: Loss = 0.030394982546567917\n",
            "Epoch 4 Step 700: Loss = 0.01004527322947979\n",
            "Epoch 4 Step 800: Loss = 0.038308627903461456\n",
            "Epoch 4 Step 900: Loss = 0.006290890276432037\n",
            "Epoch 4 Step 1000: Loss = 0.10279282182455063\n",
            "Epoch 4 Step 1100: Loss = 0.034832969307899475\n",
            "Epoch 4 Step 1200: Loss = 0.038843560963869095\n",
            "Epoch 4 Step 1300: Loss = 0.04660221189260483\n",
            "Epoch 4 Step 1400: Loss = 0.04464425891637802\n",
            "Epoch 4 Step 1500: Loss = 0.047321420162916183\n",
            "Epoch 4 Step 1600: Loss = 0.04651694372296333\n",
            "Epoch 4 Step 1700: Loss = 0.014185430482029915\n",
            "Epoch 4 Step 1800: Loss = 0.049883291125297546\n",
            "Start of epoch 5\n",
            "Epoch 5 Step 0: Loss = 0.022713422775268555\n",
            "Epoch 5 Step 100: Loss = 0.03418412432074547\n",
            "Epoch 5 Step 200: Loss = 0.021565904840826988\n",
            "Epoch 5 Step 300: Loss = 0.008300459943711758\n",
            "Epoch 5 Step 400: Loss = 0.06791358441114426\n",
            "Epoch 5 Step 500: Loss = 0.04904777929186821\n",
            "Epoch 5 Step 600: Loss = 0.0355692133307457\n",
            "Epoch 5 Step 700: Loss = 0.00607399782165885\n",
            "Epoch 5 Step 800: Loss = 0.0245341956615448\n",
            "Epoch 5 Step 900: Loss = 0.004244054667651653\n",
            "Epoch 5 Step 1000: Loss = 0.09863066673278809\n",
            "Epoch 5 Step 1100: Loss = 0.012265518307685852\n",
            "Epoch 5 Step 1200: Loss = 0.047831058502197266\n",
            "Epoch 5 Step 1300: Loss = 0.04503967985510826\n",
            "Epoch 5 Step 1400: Loss = 0.027257783338427544\n",
            "Epoch 5 Step 1500: Loss = 0.02714713290333748\n",
            "Epoch 5 Step 1600: Loss = 0.016432905569672585\n",
            "Epoch 5 Step 1700: Loss = 0.013638775795698166\n",
            "Epoch 5 Step 1800: Loss = 0.03372475132346153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dac30c9b-f5ad-402c-8417-292d3f9717bd"
      },
      "source": [
        "Adding Accuracy Metric <br/>\n",
        "\n",
        "Enhance the custom training loop by adding an accuracy metric to monitor model performance.\n",
        "\n",
        "\n",
        "1. Set up the environment and define the model, loss function, and optimizer.\n",
        "\n",
        "2. Add Sparse Categorical Accuracy as a metric.\n",
        "\n",
        "3. Implement the custom training loop with accuracy tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "\n",
        "# Define the model with a Flatten layer and two Dense layers\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10)\n",
        "])\n",
        "\n",
        "# Define Loss Function, Optimizer, and Metric\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
        "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
        "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n",
        "\n",
        "# Implement the custom training loop with accuracy\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "          logits = model(x_batch_train, training=True) # forward pass: compute predictions\n",
        "          loss_value = loss_fn(y_batch_train, logits) # compute loss\n",
        "\n",
        "        # compute grandients\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        # apply gradients to update model weights\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        # update the accuracy metric\n",
        "        accuracy_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log the loss and accuracy every 100 steps\n",
        "        if step % 100 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
        "\n",
        "    # Reset the metric at the end of each epoch\n",
        "    accuracy_metric.reset_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G-kJ2jXTOJX",
        "outputId": "481ebca4-e8ed-47dc-a83b-8b0619235c16"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Step 0: Loss = 2.4485409259796143 Accuracy = 0.09375\n",
            "Epoch 1 Step 100: Loss = 0.624505877494812 Accuracy = 0.7667078971862793\n",
            "Epoch 1 Step 200: Loss = 0.41013437509536743 Accuracy = 0.8334888219833374\n",
            "Epoch 1 Step 300: Loss = 0.2480856031179428 Accuracy = 0.8539243936538696\n",
            "Epoch 1 Step 400: Loss = 0.18354114890098572 Accuracy = 0.8673628568649292\n",
            "Epoch 1 Step 500: Loss = 0.44643551111221313 Accuracy = 0.8733158707618713\n",
            "Epoch 1 Step 600: Loss = 0.18937981128692627 Accuracy = 0.8829554915428162\n",
            "Epoch 1 Step 700: Loss = 0.11431771516799927 Accuracy = 0.890647292137146\n",
            "Epoch 1 Step 800: Loss = 0.16023290157318115 Accuracy = 0.8957163095474243\n",
            "Epoch 1 Step 900: Loss = 0.11517751216888428 Accuracy = 0.9003537893295288\n",
            "Epoch 1 Step 1000: Loss = 0.44569969177246094 Accuracy = 0.9030344486236572\n",
            "Epoch 1 Step 1100: Loss = 0.22025974094867706 Accuracy = 0.9065905809402466\n",
            "Epoch 1 Step 1200: Loss = 0.19484612345695496 Accuracy = 0.909268319606781\n",
            "Epoch 1 Step 1300: Loss = 0.18212634325027466 Accuracy = 0.9118466377258301\n",
            "Epoch 1 Step 1400: Loss = 0.2884894907474518 Accuracy = 0.9142130613327026\n",
            "Epoch 1 Step 1500: Loss = 0.13089080154895782 Accuracy = 0.9155563116073608\n",
            "Epoch 1 Step 1600: Loss = 0.21977363526821136 Accuracy = 0.9171416163444519\n",
            "Epoch 1 Step 1700: Loss = 0.10457105934619904 Accuracy = 0.9189814925193787\n",
            "Epoch 1 Step 1800: Loss = 0.19092068076133728 Accuracy = 0.9211896061897278\n",
            "Start of epoch 2\n",
            "Epoch 2 Step 0: Loss = 0.08186429738998413 Accuracy = 0.96875\n",
            "Epoch 2 Step 100: Loss = 0.17856721580028534 Accuracy = 0.9554455280303955\n",
            "Epoch 2 Step 200: Loss = 0.21613436937332153 Accuracy = 0.9609763622283936\n",
            "Epoch 2 Step 300: Loss = 0.06521584093570709 Accuracy = 0.9579526782035828\n",
            "Epoch 2 Step 400: Loss = 0.13800381124019623 Accuracy = 0.9570604562759399\n",
            "Epoch 2 Step 500: Loss = 0.21489471197128296 Accuracy = 0.9576472043991089\n",
            "Epoch 2 Step 600: Loss = 0.07767805457115173 Accuracy = 0.9587146639823914\n",
            "Epoch 2 Step 700: Loss = 0.033899955451488495 Accuracy = 0.9593437910079956\n",
            "Epoch 2 Step 800: Loss = 0.08435256034135818 Accuracy = 0.9605181217193604\n",
            "Epoch 2 Step 900: Loss = 0.044061433523893356 Accuracy = 0.960668683052063\n",
            "Epoch 2 Step 1000: Loss = 0.28515031933784485 Accuracy = 0.9608204364776611\n",
            "Epoch 2 Step 1100: Loss = 0.16395387053489685 Accuracy = 0.9613419771194458\n",
            "Epoch 2 Step 1200: Loss = 0.0795283243060112 Accuracy = 0.961698591709137\n",
            "Epoch 2 Step 1300: Loss = 0.09571219980716705 Accuracy = 0.9618802666664124\n",
            "Epoch 2 Step 1400: Loss = 0.20026040077209473 Accuracy = 0.9624821543693542\n",
            "Epoch 2 Step 1500: Loss = 0.07792079448699951 Accuracy = 0.9624625444412231\n",
            "Epoch 2 Step 1600: Loss = 0.17374654114246368 Accuracy = 0.9625624418258667\n",
            "Epoch 2 Step 1700: Loss = 0.021854910999536514 Accuracy = 0.9627425074577332\n",
            "Epoch 2 Step 1800: Loss = 0.10397212207317352 Accuracy = 0.963440477848053\n",
            "Start of epoch 3\n",
            "Epoch 3 Step 0: Loss = 0.048954639583826065 Accuracy = 1.0\n",
            "Epoch 3 Step 100: Loss = 0.0882958173751831 Accuracy = 0.9733911156654358\n",
            "Epoch 3 Step 200: Loss = 0.11947088688611984 Accuracy = 0.9748134613037109\n",
            "Epoch 3 Step 300: Loss = 0.03700112923979759 Accuracy = 0.9732142686843872\n",
            "Epoch 3 Step 400: Loss = 0.08444786816835403 Accuracy = 0.972724437713623\n",
            "Epoch 3 Step 500: Loss = 0.11934156715869904 Accuracy = 0.9735528826713562\n",
            "Epoch 3 Step 600: Loss = 0.05494345352053642 Accuracy = 0.974053680896759\n",
            "Epoch 3 Step 700: Loss = 0.014680197462439537 Accuracy = 0.9740549325942993\n",
            "Epoch 3 Step 800: Loss = 0.08089783042669296 Accuracy = 0.9746020436286926\n",
            "Epoch 3 Step 900: Loss = 0.015020026825368404 Accuracy = 0.9746462106704712\n",
            "Epoch 3 Step 1000: Loss = 0.1560026854276657 Accuracy = 0.9752122759819031\n",
            "Epoch 3 Step 1100: Loss = 0.13830551505088806 Accuracy = 0.975391685962677\n",
            "Epoch 3 Step 1200: Loss = 0.05731852352619171 Accuracy = 0.9754111170768738\n",
            "Epoch 3 Step 1300: Loss = 0.07261183112859726 Accuracy = 0.9752834439277649\n",
            "Epoch 3 Step 1400: Loss = 0.14670254290103912 Accuracy = 0.9753970503807068\n",
            "Epoch 3 Step 1500: Loss = 0.0413382314145565 Accuracy = 0.9749958515167236\n",
            "Epoch 3 Step 1600: Loss = 0.11561214923858643 Accuracy = 0.9749765992164612\n",
            "Epoch 3 Step 1700: Loss = 0.008587172254920006 Accuracy = 0.9751065373420715\n",
            "Epoch 3 Step 1800: Loss = 0.06838631629943848 Accuracy = 0.9754476547241211\n",
            "Start of epoch 4\n",
            "Epoch 4 Step 0: Loss = 0.02907484397292137 Accuracy = 1.0\n",
            "Epoch 4 Step 100: Loss = 0.05532713979482651 Accuracy = 0.9820544719696045\n",
            "Epoch 4 Step 200: Loss = 0.097843237221241 Accuracy = 0.9835199117660522\n",
            "Epoch 4 Step 300: Loss = 0.024134386330842972 Accuracy = 0.9818313717842102\n",
            "Epoch 4 Step 400: Loss = 0.0786605179309845 Accuracy = 0.9814526438713074\n",
            "Epoch 4 Step 500: Loss = 0.08057976514101028 Accuracy = 0.9822230339050293\n",
            "Epoch 4 Step 600: Loss = 0.04477865621447563 Accuracy = 0.9821651577949524\n",
            "Epoch 4 Step 700: Loss = 0.008199990727007389 Accuracy = 0.9815887808799744\n",
            "Epoch 4 Step 800: Loss = 0.05420490354299545 Accuracy = 0.9813514351844788\n",
            "Epoch 4 Step 900: Loss = 0.016788141801953316 Accuracy = 0.9815483093261719\n",
            "Epoch 4 Step 1000: Loss = 0.10563553869724274 Accuracy = 0.9821740984916687\n",
            "Epoch 4 Step 1100: Loss = 0.08131395280361176 Accuracy = 0.9822320342063904\n",
            "Epoch 4 Step 1200: Loss = 0.026760496199131012 Accuracy = 0.9819161295890808\n",
            "Epoch 4 Step 1300: Loss = 0.07453960925340652 Accuracy = 0.9819850325584412\n",
            "Epoch 4 Step 1400: Loss = 0.10130825638771057 Accuracy = 0.9820663928985596\n",
            "Epoch 4 Step 1500: Loss = 0.02325388975441456 Accuracy = 0.9818037748336792\n",
            "Epoch 4 Step 1600: Loss = 0.07259668409824371 Accuracy = 0.9818863272666931\n",
            "Epoch 4 Step 1700: Loss = 0.01208240445703268 Accuracy = 0.9820142388343811\n",
            "Epoch 4 Step 1800: Loss = 0.030226361006498337 Accuracy = 0.9822667837142944\n",
            "Start of epoch 5\n",
            "Epoch 5 Step 0: Loss = 0.016662757843732834 Accuracy = 1.0\n",
            "Epoch 5 Step 100: Loss = 0.041837990283966064 Accuracy = 0.9854578971862793\n",
            "Epoch 5 Step 200: Loss = 0.04785459488630295 Accuracy = 0.9864738583564758\n",
            "Epoch 5 Step 300: Loss = 0.008086874149739742 Accuracy = 0.985984206199646\n",
            "Epoch 5 Step 400: Loss = 0.044222038239240646 Accuracy = 0.9861284494400024\n",
            "Epoch 5 Step 500: Loss = 0.047877177596092224 Accuracy = 0.9867764711380005\n",
            "Epoch 5 Step 600: Loss = 0.06075837463140488 Accuracy = 0.9864808917045593\n",
            "Epoch 5 Step 700: Loss = 0.005484523717314005 Accuracy = 0.9861804842948914\n",
            "Epoch 5 Step 800: Loss = 0.04079410806298256 Accuracy = 0.9859940409660339\n",
            "Epoch 5 Step 900: Loss = 0.013802731409668922 Accuracy = 0.9860571622848511\n",
            "Epoch 5 Step 1000: Loss = 0.06576428562402725 Accuracy = 0.9867007732391357\n",
            "Epoch 5 Step 1100: Loss = 0.03672736510634422 Accuracy = 0.9866598844528198\n",
            "Epoch 5 Step 1200: Loss = 0.014223892241716385 Accuracy = 0.9862093925476074\n",
            "Epoch 5 Step 1300: Loss = 0.05534598231315613 Accuracy = 0.9863326549530029\n",
            "Epoch 5 Step 1400: Loss = 0.06378081440925598 Accuracy = 0.9864828586578369\n",
            "Epoch 5 Step 1500: Loss = 0.013387905433773994 Accuracy = 0.9863424301147461\n",
            "Epoch 5 Step 1600: Loss = 0.052726998925209045 Accuracy = 0.9863171577453613\n",
            "Epoch 5 Step 1700: Loss = 0.014913898892700672 Accuracy = 0.9863499402999878\n",
            "Epoch 5 Step 1800: Loss = 0.01800546981394291 Accuracy = 0.9865352511405945\n",
            "Start of epoch 6\n",
            "Epoch 6 Step 0: Loss = 0.012651821598410606 Accuracy = 1.0\n",
            "Epoch 6 Step 100: Loss = 0.018015647307038307 Accuracy = 0.989480197429657\n",
            "Epoch 6 Step 200: Loss = 0.06848353147506714 Accuracy = 0.9902052283287048\n",
            "Epoch 6 Step 300: Loss = 0.003015948925167322 Accuracy = 0.9894102811813354\n",
            "Epoch 6 Step 400: Loss = 0.05990971252322197 Accuracy = 0.9893235564231873\n",
            "Epoch 6 Step 500: Loss = 0.02543392963707447 Accuracy = 0.989895224571228\n",
            "Epoch 6 Step 600: Loss = 0.036211900413036346 Accuracy = 0.9898606538772583\n",
            "Epoch 6 Step 700: Loss = 0.002660270780324936 Accuracy = 0.9898359775543213\n",
            "Epoch 6 Step 800: Loss = 0.030314503237605095 Accuracy = 0.9897783994674683\n",
            "Epoch 6 Step 900: Loss = 0.010300815105438232 Accuracy = 0.9896642565727234\n",
            "Epoch 6 Step 1000: Loss = 0.05644237995147705 Accuracy = 0.990041196346283\n",
            "Epoch 6 Step 1100: Loss = 0.013131042011082172 Accuracy = 0.9898671507835388\n",
            "Epoch 6 Step 1200: Loss = 0.010558181442320347 Accuracy = 0.9897741675376892\n",
            "Epoch 6 Step 1300: Loss = 0.04044324904680252 Accuracy = 0.9898155331611633\n",
            "Epoch 6 Step 1400: Loss = 0.027934974059462547 Accuracy = 0.9898732900619507\n",
            "Epoch 6 Step 1500: Loss = 0.008149616420269012 Accuracy = 0.9898609519004822\n",
            "Epoch 6 Step 1600: Loss = 0.04368177801370621 Accuracy = 0.9899672269821167\n",
            "Epoch 6 Step 1700: Loss = 0.006372841075062752 Accuracy = 0.9899691343307495\n",
            "Epoch 6 Step 1800: Loss = 0.01590890996158123 Accuracy = 0.9900923371315002\n",
            "Start of epoch 7\n",
            "Epoch 7 Step 0: Loss = 0.012561486102640629 Accuracy = 1.0\n",
            "Epoch 7 Step 100: Loss = 0.01737227477133274 Accuracy = 0.9907178282737732\n",
            "Epoch 7 Step 200: Loss = 0.050554800778627396 Accuracy = 0.9920709133148193\n",
            "Epoch 7 Step 300: Loss = 0.001153389224782586 Accuracy = 0.9916943311691284\n",
            "Epoch 7 Step 400: Loss = 0.03701447695493698 Accuracy = 0.9918952584266663\n",
            "Epoch 7 Step 500: Loss = 0.015557968057692051 Accuracy = 0.9925773739814758\n",
            "Epoch 7 Step 600: Loss = 0.029017485678195953 Accuracy = 0.992980420589447\n",
            "Epoch 7 Step 700: Loss = 0.0011711582774296403 Accuracy = 0.9926890134811401\n",
            "Epoch 7 Step 800: Loss = 0.016607100144028664 Accuracy = 0.992899477481842\n",
            "Epoch 7 Step 900: Loss = 0.0024579246528446674 Accuracy = 0.9927511215209961\n",
            "Epoch 7 Step 1000: Loss = 0.03394363820552826 Accuracy = 0.9931631088256836\n",
            "Epoch 7 Step 1100: Loss = 0.006003396585583687 Accuracy = 0.9931596517562866\n",
            "Epoch 7 Step 1200: Loss = 0.011090527288615704 Accuracy = 0.9931307435035706\n",
            "Epoch 7 Step 1300: Loss = 0.0253328587859869 Accuracy = 0.9931783080101013\n",
            "Epoch 7 Step 1400: Loss = 0.004386052489280701 Accuracy = 0.9930853247642517\n",
            "Epoch 7 Step 1500: Loss = 0.003915524575859308 Accuracy = 0.9929630160331726\n",
            "Epoch 7 Step 1600: Loss = 0.023123830556869507 Accuracy = 0.9930707216262817\n",
            "Epoch 7 Step 1700: Loss = 0.012247035279870033 Accuracy = 0.9929453134536743\n",
            "Epoch 7 Step 1800: Loss = 0.008989608846604824 Accuracy = 0.9930420517921448\n",
            "Start of epoch 8\n",
            "Epoch 8 Step 0: Loss = 0.018218450248241425 Accuracy = 1.0\n",
            "Epoch 8 Step 100: Loss = 0.012970023788511753 Accuracy = 0.9922648668289185\n",
            "Epoch 8 Step 200: Loss = 0.019800541922450066 Accuracy = 0.9937810897827148\n",
            "Epoch 8 Step 300: Loss = 0.0016780380392447114 Accuracy = 0.99366694688797\n",
            "Epoch 8 Step 400: Loss = 0.03328113257884979 Accuracy = 0.9938434958457947\n",
            "Epoch 8 Step 500: Loss = 0.011147837154567242 Accuracy = 0.9943862557411194\n",
            "Epoch 8 Step 600: Loss = 0.041433364152908325 Accuracy = 0.9944363832473755\n",
            "Epoch 8 Step 700: Loss = 0.0014361386420205235 Accuracy = 0.9944276213645935\n",
            "Epoch 8 Step 800: Loss = 0.014334766194224358 Accuracy = 0.9944990873336792\n",
            "Epoch 8 Step 900: Loss = 0.0034648990258574486 Accuracy = 0.9943812489509583\n",
            "Epoch 8 Step 1000: Loss = 0.025434888899326324 Accuracy = 0.9947240352630615\n",
            "Epoch 8 Step 1100: Loss = 0.005577993579208851 Accuracy = 0.994692325592041\n",
            "Epoch 8 Step 1200: Loss = 0.0163827333599329 Accuracy = 0.9946138858795166\n",
            "Epoch 8 Step 1300: Loss = 0.024778073653578758 Accuracy = 0.9945474863052368\n",
            "Epoch 8 Step 1400: Loss = 0.003002858953550458 Accuracy = 0.9946243762969971\n",
            "Epoch 8 Step 1500: Loss = 0.0014247504295781255 Accuracy = 0.9946286082267761\n",
            "Epoch 8 Step 1600: Loss = 0.0186979491263628 Accuracy = 0.9947103261947632\n",
            "Epoch 8 Step 1700: Loss = 0.009597845375537872 Accuracy = 0.994727373123169\n",
            "Epoch 8 Step 1800: Loss = 0.01700165681540966 Accuracy = 0.9948118925094604\n",
            "Start of epoch 9\n",
            "Epoch 9 Step 0: Loss = 0.009035945869982243 Accuracy = 1.0\n",
            "Epoch 9 Step 100: Loss = 0.00425476860255003 Accuracy = 0.9935024976730347\n",
            "Epoch 9 Step 200: Loss = 0.03503742069005966 Accuracy = 0.9940920472145081\n",
            "Epoch 9 Step 300: Loss = 0.0027998031582683325 Accuracy = 0.9940822124481201\n",
            "Epoch 9 Step 400: Loss = 0.023222416639328003 Accuracy = 0.9943110942840576\n",
            "Epoch 9 Step 500: Loss = 0.014300675131380558 Accuracy = 0.9951971173286438\n",
            "Epoch 9 Step 600: Loss = 0.022325413301587105 Accuracy = 0.9954763054847717\n",
            "Epoch 9 Step 700: Loss = 0.001017805770970881 Accuracy = 0.995586633682251\n",
            "Epoch 9 Step 800: Loss = 0.012433819472789764 Accuracy = 0.9955524206161499\n",
            "Epoch 9 Step 900: Loss = 0.005642118863761425 Accuracy = 0.9955257773399353\n",
            "Epoch 9 Step 1000: Loss = 0.017344806343317032 Accuracy = 0.9957230091094971\n",
            "Epoch 9 Step 1100: Loss = 0.006494131870567799 Accuracy = 0.9957708716392517\n",
            "Epoch 9 Step 1200: Loss = 0.007407101336866617 Accuracy = 0.9958888292312622\n",
            "Epoch 9 Step 1300: Loss = 0.009508798830211163 Accuracy = 0.9958205223083496\n",
            "Epoch 9 Step 1400: Loss = 0.0037926698569208384 Accuracy = 0.9958065748214722\n",
            "Epoch 9 Step 1500: Loss = 0.008960925042629242 Accuracy = 0.9957528114318848\n",
            "Epoch 9 Step 1600: Loss = 0.01914120838046074 Accuracy = 0.9957838654518127\n",
            "Epoch 9 Step 1700: Loss = 0.0034660142846405506 Accuracy = 0.9955908060073853\n",
            "Epoch 9 Step 1800: Loss = 0.0038516907952725887 Accuracy = 0.9956448078155518\n",
            "Start of epoch 10\n",
            "Epoch 10 Step 0: Loss = 0.005230771377682686 Accuracy = 1.0\n",
            "Epoch 10 Step 100: Loss = 0.007261200807988644 Accuracy = 0.9947401285171509\n",
            "Epoch 10 Step 200: Loss = 0.005839427001774311 Accuracy = 0.9956467747688293\n",
            "Epoch 10 Step 300: Loss = 0.00014185949112288654 Accuracy = 0.9949128031730652\n",
            "Epoch 10 Step 400: Loss = 0.038662027567625046 Accuracy = 0.9954800605773926\n",
            "Epoch 10 Step 500: Loss = 0.0030999586451798677 Accuracy = 0.9956961274147034\n",
            "Epoch 10 Step 600: Loss = 0.03687421604990959 Accuracy = 0.99558025598526\n",
            "Epoch 10 Step 700: Loss = 0.0009309739689342678 Accuracy = 0.9955421090126038\n",
            "Epoch 10 Step 800: Loss = 0.009332165122032166 Accuracy = 0.9956304430961609\n",
            "Epoch 10 Step 900: Loss = 0.0023624212481081486 Accuracy = 0.9956645369529724\n",
            "Epoch 10 Step 1000: Loss = 0.032893165946006775 Accuracy = 0.995816707611084\n",
            "Epoch 10 Step 1100: Loss = 0.007481306791305542 Accuracy = 0.995941162109375\n",
            "Epoch 10 Step 1200: Loss = 0.026209000498056412 Accuracy = 0.995966911315918\n",
            "Epoch 10 Step 1300: Loss = 0.0065636951476335526 Accuracy = 0.9957965016365051\n",
            "Epoch 10 Step 1400: Loss = 0.0008928731549531221 Accuracy = 0.9958288669586182\n",
            "Epoch 10 Step 1500: Loss = 0.0004052940639667213 Accuracy = 0.9958361387252808\n",
            "Epoch 10 Step 1600: Loss = 0.014883613213896751 Accuracy = 0.9959009885787964\n",
            "Epoch 10 Step 1700: Loss = 0.009679351933300495 Accuracy = 0.9959399104118347\n",
            "Epoch 10 Step 1800: Loss = 0.003418251872062683 Accuracy = 0.9960091710090637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ff1a85e-8fe1-4d92-a580-076b8cc0f0eb"
      },
      "source": [
        "Custom Callback for Advanced Logging <br/>\n",
        "\n",
        " Implement a custom callback to log additional metrics and information during training.\n",
        "\n",
        "\n",
        "1. Set up the environment and define the model, loss function, optimizer, and metric.\n",
        "\n",
        "2. Create a custom callback to log additional metrics at the end of each epoch.\n",
        "\n",
        "3. Implement the custom training loop with the custom callback.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "\n",
        "# Define the model with a Flatten layer and two Dense layers\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10)\n",
        "])\n",
        "\n",
        "# Define Loss Function, Optimizer, and Metric\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
        "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
        "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n",
        "\n",
        "# Implement the custom training loop with accuracy\n",
        "epochs = 2\n",
        "custom_callback = CustomCallback()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "          # Forward pass\n",
        "          logits = model(x_batch_train, training=True)\n",
        "          # Compute loss\n",
        "          loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # compute gradients\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        # apply gradients to update model weights\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # update the accuracy metric\n",
        "        accuracy_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log the loss every 200 steps\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
        "\n",
        "    #  Call the custom callback at the end of each step\n",
        "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
        "\n",
        "    # Reset the metric at the end of each epoch\n",
        "    accuracy_metric.reset_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-kUL9aUkUH",
        "outputId": "974d4716-1faf-4f10-b384-0cd64c9a2e84"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Step 0: Loss = 2.4378397464752197 Accuracy = 0.125\n",
            "Epoch 1 Step 200: Loss = 0.39522427320480347 Accuracy = 0.8260261416435242\n",
            "Epoch 1 Step 400: Loss = 0.1886368691921234 Accuracy = 0.8631545901298523\n",
            "Epoch 1 Step 600: Loss = 0.17917487025260925 Accuracy = 0.8811355829238892\n",
            "Epoch 1 Step 800: Loss = 0.15405292809009552 Accuracy = 0.89454585313797\n",
            "Epoch 1 Step 1000: Loss = 0.4464195966720581 Accuracy = 0.9023476243019104\n",
            "Epoch 1 Step 1200: Loss = 0.19408738613128662 Accuracy = 0.9090341329574585\n",
            "Epoch 1 Step 1400: Loss = 0.25699207186698914 Accuracy = 0.9139900207519531\n",
            "Epoch 1 Step 1600: Loss = 0.23609337210655212 Accuracy = 0.9176881909370422\n",
            "Epoch 1 Step 1800: Loss = 0.13588860630989075 Accuracy = 0.9218316078186035\n",
            "End of epoch 1, loss: 0.041016072034835815, accuracy: 0.9237333536148071\n",
            "Start of epoch 2\n",
            "Epoch 2 Step 0: Loss = 0.0747869610786438 Accuracy = 1.0\n",
            "Epoch 2 Step 200: Loss = 0.14780940115451813 Accuracy = 0.9626865386962891\n",
            "Epoch 2 Step 400: Loss = 0.10120954364538193 Accuracy = 0.9586970210075378\n",
            "Epoch 2 Step 600: Loss = 0.07468225061893463 Accuracy = 0.9602745175361633\n",
            "Epoch 2 Step 800: Loss = 0.08305827528238297 Accuracy = 0.9612203240394592\n",
            "Epoch 2 Step 1000: Loss = 0.2992170751094818 Accuracy = 0.9618818759918213\n",
            "Epoch 2 Step 1200: Loss = 0.11692527681589127 Accuracy = 0.9625572562217712\n",
            "Epoch 2 Step 1400: Loss = 0.1296350359916687 Accuracy = 0.9634413123130798\n",
            "Epoch 2 Step 1600: Loss = 0.1974508911371231 Accuracy = 0.9636945724487305\n",
            "Epoch 2 Step 1800: Loss = 0.09728649258613586 Accuracy = 0.9643080234527588\n",
            "End of epoch 2, loss: 0.036235272884368896, accuracy: 0.9650333523750305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdaa30a2-3560-4d90-b4d2-53346de3bc48"
      },
      "source": [
        "Hyperparameter Tuning <br/>\n",
        "\n",
        "Add functionality to save the results of each hyperparameter tuning iteration as JSON files in a specified directory.\n",
        "\n",
        "\n",
        "Modify the tuning loop to save each iteration's results as JSON files.\n",
        "\n",
        "Specify the directory where these JSON files will be stored for easier retrieval and analysis of tuning results.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oOG8ZeCJYa9G",
        "outputId": "5bfb3f3b-f3e3-4cf8-c88b-e4459eac4629"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.4.26)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Load your dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Define the model-building function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    # Tune the number of units in the first Dense layer\n",
        "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
        "                    activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification example\n",
        "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize a Keras Tuner RandomSearch tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,  # Set the number of trials\n",
        "    executions_per_trial=1,  # Set how many executions per trial\n",
        "    directory='tuner_results',  # Directory for saving logs\n",
        "    project_name='hyperparam_tuning'\n",
        ")\n",
        "\n",
        "# Run the tuner search (make sure the data is correct)\n",
        "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
        "\n",
        "# Create the directory for saving results if it doesn't exist\n",
        "results_dir = 'tuning_results_json' # Using a different directory name to avoid conflict with tuner's directory\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Save the tuning results as JSON files\n",
        "try:\n",
        "    for i in range(10):\n",
        "        # Fetch the best hyperparameters from the tuner\n",
        "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "        # Results dictionary to save hyperparameters and score\n",
        "        results = {\n",
        "            \"trial\": i + 1,\n",
        "            \"hyperparameters\": best_hps.values,  # Hyperparameters tuned in this trial\n",
        "            \"score\": None  # Add any score or metrics if available\n",
        "        }\n",
        "\n",
        "        # Save the results as JSON\n",
        "        with open(os.path.join('tuning_results_json', f\"trial_{i + 1}.json\"), \"w\") as f:\n",
        "            json.dump(results, f)\n",
        "\n",
        "except IndexError:\n",
        "    print(\"Tuning process has not completed or no results available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6RklySiTdMd",
        "outputId": "7ba5a878-b160-4001-84f6-d7b287c07631"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from tuner_results/hyperparam_tuning/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87767081-4566-4f33-9e70-33ece4894793"
      },
      "source": [
        "Explanation of Hyperparameter Tuning <br/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: \"num_trials specifies the number of top hyperparameter sets to return. Setting num_trials=1 means that it will return only the best set of hyperparameters found during the tuning process.\""
      ],
      "metadata": {
        "id": "bVCx4WOYZ8Q2"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "prev_pub_hash": "48a1eb2565c8b635156cd21708473ccadb84e292e93f3530a9d5223b7590344e",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}